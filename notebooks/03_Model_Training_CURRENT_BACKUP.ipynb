{
<<<<<<< Updated upstream
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae225fc",
   "metadata": {},
   "source": [
    "#### 03_Model_Training and Evaluation: Hospital Readmission Prediction for Diabetic Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3506de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variables for PySpark on Windows \n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-17.0.12\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea7efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "from logging import config\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiabeticReadmission\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.security.authentication\", \"simple\") \\\n",
    "    .config(\"spark.hadoop.security.authorization\", \"false\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8565cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and functions \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbac46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 97805 rows\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "data_path = r\"C:\\Projects\\hospital_readmission_prediction\\data\\output\\ml_ready_data.csv\"\n",
    "df_pandas = pd.read_csv(data_path)\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "print(f\"Data loaded: {df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c61aa6",
   "metadata": {},
   "source": [
    "Reloading the output from feature engineering step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d006f2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vector conversion from String to VectorUDT...\n",
      "Conversion complete and dataset cached!\n",
      "Dataset count: 97805 rows\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- readmitted: long (nullable = true)\n",
      "\n",
      "+--------------------+----------+\n",
      "|            features|readmitted|\n",
      "+--------------------+----------+\n",
      "|[1.08009058227626...|         0|\n",
      "|[1.08009058227626...|         0|\n",
      "|[1.08009058227626...|         0|\n",
      "|[-0.9258388064697...|         0|\n",
      "|[-0.9258388064697...|         0|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "try:\n",
    "    df_raw = spark.createDataFrame(df_pandas)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error creating Spark DataFrame from Pandas: {e}\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "if 'features' in df_raw.columns and 'readmitted' in df_raw.columns:\n",
    "    print(\"Starting vector conversion from String to VectorUDT...\")\n",
    "\n",
    "    def fast_vector_parse(vec_str):\n",
    "        \"\"\"Convert '[v1,v2,...]' string to Spark Dense Vector.\"\"\"\n",
    "        if not vec_str or vec_str == '[]':\n",
    "            return None\n",
    "        try:\n",
    "            cleaned = vec_str[1:-1]  # remove brackets\n",
    "            values = np.fromstring(cleaned, sep=',', dtype=np.float64)\n",
    "            return Vectors.dense(values.tolist())\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    vector_udf = udf(fast_vector_parse, VectorUDT())\n",
    "\n",
    "    df_ml = df_raw.repartition(8) \\\n",
    "                  .withColumn(\"features_vec\", vector_udf(\"features\")) \\\n",
    "                  .filter(\"features_vec is not null\")\n",
    "\n",
    "    df = df_ml.select(\"features_vec\", \"readmitted\") \\\n",
    "              .withColumnRenamed(\"features_vec\", \"features\") \\\n",
    "              .cache()\n",
    "\n",
    "    print(\"Conversion complete and dataset cached!\")\n",
    "    print(f\"Dataset count: {df.count()} rows\")\n",
    "    df.printSchema()\n",
    "    df.show(5, truncate=True)\n",
    "else:\n",
    "    raise ValueError(\"Required columns 'features' or 'readmitted' not found in df_raw.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9c0db",
   "metadata": {},
   "source": [
    "Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069380c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLASS DISTRIBUTION:\n",
      "  Not Readmitted (0): 86,599 (88.5%)\n",
      "  Readmitted (1): 11,206 (11.5%)\n",
      "  Total: 97,805 rows\n",
      "  Imbalance Ratio: 7.73:1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Most efficient approach - single action\n",
    "class_counts = df.agg(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.sum(F.col(\"readmitted\")).alias(\"readmitted_count\")\n",
    ").collect()[0]\n",
    "\n",
    "total = class_counts[\"total\"]\n",
    "readmitted = class_counts[\"readmitted_count\"]\n",
    "not_readmitted = total - readmitted\n",
    "readmitted_rate = readmitted / total\n",
    "\n",
    "print(f\"\\nCLASS DISTRIBUTION:\")\n",
    "print(f\"  Not Readmitted (0): {not_readmitted:,} ({(1-readmitted_rate)*100:.1f}%)\")\n",
    "print(f\"  Readmitted (1): {readmitted:,} ({readmitted_rate*100:.1f}%)\")\n",
    "print(f\"  Total: {total:,} rows\")\n",
    "print(f\"  Imbalance Ratio: {not_readmitted/readmitted:.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82425106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (78427, 1)\n",
      "y_train: (78427, 1)\n",
      "X_test: (19378, 1)\n",
      "y_test: (19378, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into features vector and separate the target\n",
    "X = df.select(\"features\")  # This is your feature matrix\n",
    "y = df.select(\"readmitted\")  # This is your target\n",
    "\n",
    "# Equivalent to: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=1)\n",
    "\n",
    "# Create the separated variables\n",
    "X_train = train_data.select(\"features\")\n",
    "y_train = train_data.select(\"readmitted\")\n",
    "X_test = test_data.select(\"features\")\n",
    "y_test = test_data.select(\"readmitted\")\n",
    "\n",
    "# Print equivalent output with exact counts\n",
    "print(f\"X_train: ({X_train.count()}, {len(X_train.columns)})\")\n",
    "print(f\"y_train: ({y_train.count()}, {len(y_train.columns)})\")\n",
    "print(f\"X_test: ({X_test.count()}, {len(X_test.columns)})\")\n",
    "print(f\"y_test: ({y_test.count()}, {len(y_test.columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835504d",
   "metadata": {},
   "source": [
    "Create schema for metrics DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2187eab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores DataFrame reset!\n",
      "+----------+--------------+-------------+--------+---------------+------------+-------------+-------+--------------+-----------+------------+\n",
      "|Model_Name|Train_Accuracy|Test_Accuracy|Train_f1|Train_precision|Train_recall|Train_auc_roc|Test_f1|Test_precision|Test_recall|Test_auc_roc|\n",
      "+----------+--------------+-------------+--------+---------------+------------+-------------+-------+--------------+-----------+------------+\n",
      "+----------+--------------+-------------+--------+---------------+------------+-------------+-------+--------------+-----------+------------+\n",
      "\n",
      "root\n",
      " |-- Model_Name: string (nullable = true)\n",
      " |-- Train_Accuracy: double (nullable = true)\n",
      " |-- Test_Accuracy: double (nullable = true)\n",
      " |-- Train_f1: double (nullable = true)\n",
      " |-- Train_precision: double (nullable = true)\n",
      " |-- Train_recall: double (nullable = true)\n",
      " |-- Train_auc_roc: double (nullable = true)\n",
      " |-- Test_f1: double (nullable = true)\n",
      " |-- Test_precision: double (nullable = true)\n",
      " |-- Test_recall: double (nullable = true)\n",
      " |-- Test_auc_roc: double (nullable = true)\n",
      "\n",
      "Scores DataFrame reset!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Model_Name: string, Train_Accuracy: double, Test_Accuracy: double, Train_f1: double, Train_precision: double, Train_recall: double, Train_auc_roc: double, Test_f1: double, Test_precision: double, Test_recall: double, Test_auc_roc: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define schema for the scores DataFrame\n",
    "\n",
    "# Add this at the beginning of your model training section\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def reset_scores():\n",
    "    global scores\n",
    "        \n",
    "    schema = StructType([\n",
    "        StructField(\"Model_Name\", StringType(), True),\n",
    "        StructField(\"Train_Accuracy\", DoubleType(), True),\n",
    "        StructField(\"Test_Accuracy\", DoubleType(), True),\n",
    "        StructField(\"Train_f1\", DoubleType(), True),\n",
    "        StructField(\"Train_precision\", DoubleType(), True),\n",
    "        StructField(\"Train_recall\", DoubleType(), True),\n",
    "        StructField(\"Train_auc_roc\", DoubleType(), True),\n",
    "        StructField(\"Test_f1\", DoubleType(), True),\n",
    "        StructField(\"Test_precision\", DoubleType(), True),\n",
    "        StructField(\"Test_recall\", DoubleType(), True),\n",
    "        StructField(\"Test_auc_roc\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    scores = spark.createDataFrame([], schema)\n",
    "    print(\"Scores DataFrame reset!\")\n",
    "    return scores\n",
    "\n",
    "scores = reset_scores()\n",
    "\n",
    "scores.show()\n",
    "scores.printSchema()\n",
    "# Call this before training models\n",
    "reset_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "893d094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define helper function for model training and evaluation\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.classification import ClassificationModel\n",
    "\n",
    "def train_and_evaluate(model, train_df: DataFrame, test_df: DataFrame, model_name: str):\n",
    "    # Check if model is already fitted\n",
    "    if hasattr(model, \"transform\") and not hasattr(model, \"fit\"):\n",
    "        fitted_model = model  # already fitted\n",
    "    else:\n",
    "        fitted_model = model.fit(train_df)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = fitted_model.transform(train_df)\n",
    "    test_pred = fitted_model.transform(test_df)\n",
    "    \n",
    "    # Evaluators\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"readmitted\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"readmitted\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "    precision_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"readmitted\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    recall_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"readmitted\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    roc_evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"readmitted\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"Model_Name\": model_name,\n",
    "        \"Train_Accuracy\": acc_evaluator.evaluate(train_pred),\n",
    "        \"Test_Accuracy\": acc_evaluator.evaluate(test_pred),\n",
    "        \"Train_f1\": f1_evaluator.evaluate(train_pred),\n",
    "        \"Train_precision\": precision_evaluator.evaluate(train_pred),\n",
    "        \"Train_recall\": recall_evaluator.evaluate(train_pred),\n",
    "        \"Train_auc_roc\": roc_evaluator.evaluate(train_pred),\n",
    "        \"Test_f1\": f1_evaluator.evaluate(test_pred),\n",
    "        \"Test_precision\": precision_evaluator.evaluate(test_pred),\n",
    "        \"Test_recall\": recall_evaluator.evaluate(test_pred),\n",
    "        \"Test_auc_roc\": roc_evaluator.evaluate(test_pred),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a75dee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model_Name': 'LogisticRegression', 'Train_Accuracy': 0.883777270582835, 'Test_Accuracy': 0.8878109195995458, 'Train_f1': 0.8310405161383769, 'Train_precision': 0.8170444671227207, 'Train_recall': 0.8837772705828351, 'Train_auc_roc': 0.6459136066271668, 'Test_f1': 0.8373437014130324, 'Test_precision': 0.8189700585227396, 'Test_recall': 0.887810919599546, 'Test_auc_roc': 0.6352764277001627}\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|        Model_Name|   Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|     Train_auc_roc|           Test_f1|    Test_precision|      Test_recall|      Test_auc_roc|\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|LogisticRegression|0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351|0.6459136066271668|0.8373437014130324|0.8189700585227396|0.887810919599546|0.6352764277001627|\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression - Base Model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Base Logistic Regression\n",
    "lr_base = LogisticRegression(featuresCol=\"features\", labelCol=\"readmitted\", maxIter=50)\n",
    "lr_metrics = train_and_evaluate(lr_base, train_data, test_data, \"LogisticRegression\")\n",
    "\n",
    "print(lr_metrics)\n",
    "#covert dictionary to row and append to scores DataFrame\n",
    "scores = scores.union(spark.createDataFrame([Row(**lr_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e633e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|            features|readmitted|      class_weight|\n",
      "+--------------------+----------+------------------+\n",
      "|[-0.9258388064697...|         0|1.1294010323444843|\n",
      "|[-0.9258388064697...|         0|1.1294010323444843|\n",
      "|[-0.9258388064697...|         0|1.1294010323444843|\n",
      "|[-0.9258388064697...|         0|1.1294010323444843|\n",
      "|[-0.9258388064697...|         0|1.1294010323444843|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression - Balanced\n",
    "# Class Weighting\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = df.groupBy(\"readmitted\").count().collect()\n",
    "total = sum(row['count'] for row in class_counts)\n",
    "weights = {row['readmitted']: total/row['count'] for row in class_counts}\n",
    "\n",
    "# Define a UDF to map readmitted -> weight\n",
    "get_weight = udf(lambda x: float(weights[x]), DoubleType())\n",
    "\n",
    "# Add a weight column to the training data\n",
    "train_data_bal = train_data.withColumn(\"class_weight\", get_weight(col(\"readmitted\")))\n",
    "\n",
    "# Check\n",
    "train_data_bal.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0fbdf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|     Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|      Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351|0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546|0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583|0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385|0.6375553473800778|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train Logistic Regression with class weights\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logreg_bal = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    weightCol=\"class_weight\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "logreg_bal_metrics = train_and_evaluate(logreg_bal, train_data_bal, test_data, \"LogisticReg_Balanced\")\n",
    "# Append results to scores DataFrame\n",
    "scores = scores.union(spark.createDataFrame([Row(**logreg_bal_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ba8eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|     Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|      Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351|0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546|0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583|0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385|0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435|0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844|0.6354284303420535|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression Unbalanced\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "#Initialize logistic regression model\n",
    "logreg_unbal = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "#train and evaluate\n",
    "logreg_unbal_metrics = train_and_evaluate(logreg_unbal, train_data, test_data, \"LogisticReg_Unbalanced\")\n",
    "\n",
    "#Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**logreg_unbal_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f1d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "#Initialize Decision Tree Classifier\n",
    "dt_base = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"readmitted\", maxDepth=5, seed=1)\n",
    "#Train and evaluate\n",
    "dt_base_metrics = train_and_evaluate(dt_base, train_data, test_data, \"DecisionTree\")\n",
    "#Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**dt_base_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355e5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree_Balanced\n",
    "#calculate class weights\n",
    "weight_col_name=\"class_weight\"\n",
    "train_counts = train_data.groupBy(\"readmitted\").count().collect()\n",
    "total_train = sum(row['count'] for row in train_counts)\n",
    "num_classes = len(train_counts)\n",
    "weight_dict = {row['readmitted']: total_train / (num_classes * row['count']) for row in train_counts}\n",
    "\n",
    "#Add a weight column to the training data\n",
    "weight_udf = udf(lambda x: float(weight_dict[x]), DoubleType())\n",
    "train_balanced = train_data.withColumn(weight_col_name, weight_udf(col(\"readmitted\")))\n",
    "#Initialize Decision Tree Classifier with class weights\n",
    "dt_balanced = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"readmitted\", weightCol=weight_col_name, maxDepth=5, seed=1)\n",
    "\n",
    "#Train and evaluate\n",
    "dt_balanced_metrics = train_and_evaluate(dt_balanced, train_balanced, test_data, \"DecisionTree_Balanced\")\n",
    "#Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**dt_balanced_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42985f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Tuned \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "dt_tuned = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt_tuned.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(dt_tuned.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"readmitted\", metricName=\"areaUnderROC\")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=dt_tuned,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Fit the cross-validated model\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Evaluate on train and test\n",
    "best_dt = cv_model.bestModel\n",
    "dt_tuned_metrics = train_and_evaluate(best_dt, train_data, test_data, \"DecisionTree_Tuned\")\n",
    "#Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**dt_tuned_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882b63e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Base Random Forest\n",
    "rf_base = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    numTrees=100,\n",
    "    maxDepth=5,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "rf_base_metrics = train_and_evaluate(rf_base, train_data, test_data, \"RandomForest\")\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**rf_base_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f1e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Balanced\n",
    "# Add weight column based on training class distribution\n",
    "train_balanced = train_data.withColumn(weight_col_name, weight_udf(col(\"readmitted\")))\n",
    "\n",
    "# Random Forest with weights\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    weightCol=weight_col_name,\n",
    "    numTrees=100,\n",
    "    maxDepth=5,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "rf_balanced_metrics = train_and_evaluate(rf_balanced, train_balanced, test_data, \"RandomForest_Balanced\")\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**rf_balanced_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c9ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest tuned\n",
    "rf_tuned = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf_tuned.numTrees, [50, 100, 150]) \\\n",
    "    .addGrid(rf_tuned.maxDepth, [5, 7, 10]) \\\n",
    "    .addGrid(rf_tuned.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "cv_rf = CrossValidator(\n",
    "    estimator=rf_tuned,\n",
    "    estimatorParamMaps=param_grid_rf,\n",
    "    evaluator=evaluator,  # BinaryClassificationEvaluator defined earlier\n",
    "    numFolds=3,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Fit cross-validated model\n",
    "cv_rf_model = cv_rf.fit(train_data)\n",
    "\n",
    "# Best model evaluation\n",
    "best_rf = cv_rf_model.bestModel\n",
    "rf_tuned_metrics = train_and_evaluate(best_rf, train_data, test_data, \"RandomForest_Tuned\")\n",
    "\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**rf_tuned_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79130540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#Define evaluators ---\n",
    "binary_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"readmitted\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Base XGBoost\n",
    "xgb_base = GBTClassifier(featuresCol=\"features\",labelCol=\"readmitted\",maxDepth=5,\n",
    "    maxIter=100,stepSize=0.1,seed=1)\n",
    "\n",
    "# Define hyperparameter grid for CV\n",
    "param_grid_xgb = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_base.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(xgb_base.maxIter, [50, 100, 150]) \\\n",
    "    .addGrid(xgb_base.stepSize, [0.05, 0.1, 0.2]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=xgb_base,\n",
    "    estimatorParamMaps=param_grid_xgb,\n",
    "    evaluator=binary_eval,\n",
    "    numFolds=3,\n",
    "    parallelism=2,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Fit cross-validated model\n",
    "cv_xgb_model = cv_xgb.fit(train_data)\n",
    "\n",
    "# Get best model\n",
    "best_xgb = cv_xgb_model.bestModel\n",
    "\n",
    "# Train and evaluate\n",
    "xgb_base_metrics = train_and_evaluate(xgb_base, train_data, test_data, \"XGBoost\")\n",
    "\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**xgb_base_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4a8f3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Balanced\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "'''# Weighted training for imbalance\n",
    "# Step 1: Define class weights (assuming 0 = majority, 1 = minority)\n",
    "# Adjust ratio if your data imbalance is different (e.g., 0.70.3)'''\n",
    "neg_weight = 1.0\n",
    "pos_weight = 2.0  # boost minority class\n",
    "\n",
    "# Step 2: Create a weight column\n",
    "train_balanced = train_data.withColumn(\n",
    "    \"classWeightCol\",\n",
    "    when(col(\"readmitted\") == 1, pos_weight).otherwise(neg_weight)\n",
    ")\n",
    "\n",
    "# Step 3: Define GBT model (XGBoost-like)\n",
    "xgb_balanced = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    weightCol=\"classWeightCol\",  # handle imbalance\n",
    "    maxDepth=5,\n",
    "    maxIter=100,\n",
    "    stepSize=0.1,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Step 4: Train and evaluate\n",
    "xgb_balanced_metrics = train_and_evaluate(xgb_balanced, train_balanced, test_data, \"XGBoost_Balanced\")\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**xgb_balanced_metrics)]))\n",
    "scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4348fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Tuned\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define evaluator (AUC-based)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"readmitted\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Step 1: Base model (GBT acts as XGBoost substitute)\n",
    "xgb_tuned_base = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    maxIter=100,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Step 2: Hyperparameter grid (GBT params)\n",
    "param_grid_xgb = ParamGridBuilder() \\\n",
    "    .addGrid(xgb_tuned_base.maxDepth, [3, 5, 7]) \\\n",
    "    .addGrid(xgb_tuned_base.maxIter, [50, 100, 150]) \\\n",
    "    .addGrid(xgb_tuned_base.stepSize, [0.05, 0.1, 0.2]) \\\n",
    "    .build()\n",
    "\n",
    "# Step 3: Cross-validation setup\n",
    "cv_xgb = CrossValidator(\n",
    "    estimator=xgb_tuned_base,\n",
    "    estimatorParamMaps=param_grid_xgb,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Step 4: Fit cross-validated model\n",
    "cv_xgb_model = cv_xgb.fit(train_data)\n",
    "\n",
    "# Step 5: Evaluate best model\n",
    "best_xgb = cv_xgb_model.bestModel\n",
    "xgb_tuned_metrics = train_and_evaluate(best_xgb, train_data, test_data, \"XGBoost_Tuned\")\n",
    "\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**xgb_tuned_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe72a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lightgbm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e99650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "|      LinearSVC_Base|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.5459514837109436|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.5414026320731591|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base Linear SVC\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Base SVC\n",
    "svc_base = LinearSVC(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "svc_base_metrics = train_and_evaluate(svc_base, train_data, test_data, \"LinearSVC_Base\")\n",
    "scores = scores.union(spark.createDataFrame([Row(**svc_base_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c2ab0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "|      LinearSVC_Base|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.5459514837109436|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.5414026320731591|\n",
      "|  LinearSVC_Balanced|0.5953168044077135|0.6797915161523377|0.5902862531733137|0.5999359092647067|0.5953168044077135| 0.6449860406549798|0.7356167587195362|0.8314821252714105|0.6797915161523377| 0.6316394963124956|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC Balanced (class weighting)\n",
    "# Count classes\n",
    "count_0 = train_data.filter(\"readmitted = 0\").count()\n",
    "count_1 = train_data.filter(\"readmitted = 1\").count()\n",
    "\n",
    "# Downsample majority class\n",
    "ratio = count_1 / count_0\n",
    "majority_df = train_data.filter(\"readmitted = 0\").sample(withReplacement=False, fraction=ratio, seed=1)\n",
    "minority_df = train_data.filter(\"readmitted = 1\")\n",
    "\n",
    "train_balanced = majority_df.union(minority_df)\n",
    "\n",
    "# Train SVC on balanced data\n",
    "svc_balanced = LinearSVC(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "svc_balanced_metrics = train_and_evaluate(svc_balanced, train_balanced, test_data, \"LinearSVC_Balanced\")\n",
    "scores = scores.union(spark.createDataFrame([Row(**svc_balanced_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8e85cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "|      LinearSVC_Base|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.5459514837109436|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.5414026320731591|\n",
      "|  LinearSVC_Balanced|0.5953168044077135|0.6797915161523377|0.5902862531733137|0.5999359092647067|0.5953168044077135| 0.6449860406549798|0.7356167587195362|0.8314821252714105|0.6797915161523377| 0.6316394963124956|\n",
      "|     LinearSVC_Tuned|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628|  0.551999224055471|0.8367581293315685|0.7903171463562303|0.8889978325936629|  0.551653074945304|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC Tuned (Cross-Validation)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Base model for CV\n",
    "svc = LinearSVC(featuresCol=\"features\", labelCol=\"readmitted\")\n",
    "\n",
    "# Param grid\n",
    "paramGrid_svc = ParamGridBuilder() \\\n",
    "    .addGrid(svc.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(svc.maxIter, [50, 100, 150]) \\\n",
    "    .build()\n",
    "\n",
    "# Evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"readmitted\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_svc = CrossValidator(\n",
    "    estimator=svc,\n",
    "    estimatorParamMaps=paramGrid_svc,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "cv_svc_model = cv_svc.fit(train_data)\n",
    "best_svc = cv_svc_model.bestModel\n",
    "\n",
    "# Evaluate\n",
    "svc_tuned_metrics = train_and_evaluate(best_svc, train_data, test_data, \"LinearSVC_Tuned\")\n",
    "scores = scores.union(spark.createDataFrame([Row(**svc_tuned_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23a7e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "|      LinearSVC_Base|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.5459514837109436|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.5414026320731591|\n",
      "|  LinearSVC_Balanced|0.5953168044077135|0.6797915161523377|0.5902862531733137|0.5999359092647067|0.5953168044077135| 0.6449860406549798|0.7356167587195362|0.8314821252714105|0.6797915161523377| 0.6316394963124956|\n",
      "|     LinearSVC_Tuned|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628|  0.551999224055471|0.8367581293315685|0.7903171463562303|0.8889978325936629|  0.551653074945304|\n",
      "|            LightGBM|0.8850013388246395|0.8886882031169367|0.8323647740636001|0.8597180707850168|0.8850013388246395| 0.6694776619130313|0.8376930298608374|0.8342877203814565|0.8886882031169367| 0.6477609653275563|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "lgb_proxy = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    maxIter=100,\n",
    "    seed=1)\n",
    "\n",
    "#Define hyperparameter grid\n",
    "paramGrid_lgb = ParamGridBuilder()\\\n",
    "    .addGrid(lgb_proxy.maxDepth, [3,5,7])\\\n",
    "    .addGrid(lgb_proxy.maxIter, [50,100])\\\n",
    "    .addGrid(lgb_proxy.stepSize, [0.05,0.1])\\\n",
    "    .build()\n",
    "\n",
    "# Define evalautor\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"readmitted\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\")\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_lgb = CrossValidator(\n",
    "    estimator=lgb_proxy,\n",
    "    estimatorParamMaps=paramGrid_lgb,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=1)\n",
    "# Fit cross-validated model\n",
    "cv_lgb_model = cv_lgb.fit(train_data)\n",
    "best_lgb = cv_lgb_model.bestModel\n",
    "\n",
    "# Train and evaluate\n",
    "lgb_metrics = train_and_evaluate(best_lgb, train_data, test_data, \"LightGBM\")\n",
    "scores = scores.union(spark.createDataFrame([Row(**lgb_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05316f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "|      LinearSVC_Base|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.5459514837109436|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.5414026320731591|\n",
      "|  LinearSVC_Balanced|0.5953168044077135|0.6797915161523377|0.5902862531733137|0.5999359092647067|0.5953168044077135| 0.6449860406549798|0.7356167587195362|0.8314821252714105|0.6797915161523377| 0.6316394963124956|\n",
      "|     LinearSVC_Tuned|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628|  0.551999224055471|0.8367581293315685|0.7903171463562303|0.8889978325936629|  0.551653074945304|\n",
      "|            LightGBM|0.8850013388246395|0.8886882031169367|0.8323647740636001|0.8597180707850168|0.8850013388246395| 0.6694776619130313|0.8376930298608374|0.8342877203814565|0.8886882031169367| 0.6477609653275563|\n",
      "|   LightGBM_Balanced|0.6627436979611613|0.6341211683352255|0.7231050469058541|0.8560357706973183|0.6627436979611613| 0.7380140465349575|0.7017172349648702|0.8373573581348537|0.6341211683352255| 0.6394412191278451|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LightGBM Balanced\n",
    "# Create a weight column if not already created\n",
    "# Example: inverse class frequency\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "train_balanced = train_data.withColumn(\n",
    "    \"weight\",\n",
    "    when(col(\"readmitted\") == 1, total / (2 * readmitted))\n",
    "    .otherwise(total / (2 * not_readmitted))\n",
    ")\n",
    "\n",
    "lgb_balanced_proxy = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"readmitted\",\n",
    "    weightCol=\"weight\",\n",
    "    maxDepth=5,\n",
    "    maxIter=100,\n",
    "    stepSize=0.1,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "lgb_balanced_metrics = train_and_evaluate(lgb_balanced_proxy, train_balanced, test_data, \"LightGBM_Balanced\")\n",
    "scores = scores.union(spark.createDataFrame([Row(**lgb_balanced_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "771fd86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|          Model_Name|    Train_Accuracy|     Test_Accuracy|          Train_f1|   Train_precision|      Train_recall|      Train_auc_roc|           Test_f1|    Test_precision|       Test_recall|       Test_auc_roc|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "|  LogisticRegression| 0.883777270582835|0.8878109195995458|0.8310405161383769|0.8170444671227207|0.8837772705828351| 0.6459136066271668|0.8373437014130324|0.8189700585227396| 0.887810919599546| 0.6352764277001627|\n",
      "|LogisticReg_Balanced|0.6413225037295829|0.6407782020848385|0.7052911794213521|0.8326522788982762| 0.641322503729583| 0.6486930694758973|0.7068051475905784|  0.83534132063233|0.6407782020848385| 0.6375553473800778|\n",
      "|LogisticReg_Unbal...|0.8838282734262435|0.8880689441634844|0.8306790819885745|0.8114497220727238|0.8838282734262435| 0.6462027845427367|0.8369867650415892|0.8147573116460428|0.8880689441634844| 0.6354284303420535|\n",
      "|        DecisionTree|0.8849248345595266|0.8887398080297244|0.8318180754151242| 0.864038117188065|0.8849248345595268|0.39364286964415285|0.8372267725476715|0.8296832365846959|0.8887398080297244|0.39748646326405823|\n",
      "|DecisionTree_Bala...|0.6448416999247708| 0.638817215398906|0.7081491458754136|0.8350230042779925|0.6448416999247708| 0.3981165913407315|0.7052833494130711| 0.835236676754175|0.6388172153989059| 0.4006030369169822|\n",
      "|  DecisionTree_Tuned|0.8846570696316319|0.8888430178552998|0.8312683510984235|0.8489768782094819|0.8846570696316319|  0.415862264576803| 0.837080284502406|0.8308091334112185|0.8888430178552998| 0.4208340960452137|\n",
      "|        RandomForest|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.6498335465103302|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6306237435494004|\n",
      "|RandomForest_Bala...|0.6173766687492828|0.6145113014759005|0.6863835379572903|0.8364206177182681|0.6173766687492828| 0.6518487528707894|0.6861365707521825| 0.837522469513594|0.6145113014759006| 0.6301991616470711|\n",
      "|  RandomForest_Tuned| 0.884567814655667|0.8889978325936629|0.8304124922021183|0.8978927438736747| 0.884567814655667| 0.7350054960216884|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.6425785590538157|\n",
      "|             XGBoost| 0.887015951139276|0.8887398080297244|0.8370451003203311|0.8854475942647844| 0.887015951139276| 0.7093905509881139|0.8394543411101744|0.8424262764727373|0.8887398080297244| 0.6505326623249909|\n",
      "|    XGBoost_Balanced|0.8876662373927346|0.8862111673031272|0.8449017253247438|0.8610395443561074|0.8876662373927346|  0.721821426862365|0.8422167604493133|0.8335542193939911|0.8862111673031272| 0.6491297717191534|\n",
      "|       XGBoost_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751269120301903|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488120706802434|\n",
      "|      LinearSVC_Base|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628| 0.5459514837109436|0.8367581293315685|0.7903171463562303|0.8889978325936629| 0.5414026320731591|\n",
      "|  LinearSVC_Balanced|0.5953168044077135|0.6797915161523377|0.5902862531733137|0.5999359092647067|0.5953168044077135| 0.6449860406549798|0.7356167587195362|0.8314821252714105|0.6797915161523377| 0.6316394963124956|\n",
      "|     LinearSVC_Tuned|0.8845423132339628|0.8889978325936629|0.8303502642597915|  0.78241510390129|0.8845423132339628|  0.551999224055471|0.8367581293315685|0.7903171463562303|0.8889978325936629|  0.551653074945304|\n",
      "|            LightGBM|0.8850013388246395|0.8886882031169367|0.8323647740636001|0.8597180707850168|0.8850013388246395| 0.6694776619130313|0.8376930298608374|0.8342877203814565|0.8886882031169367| 0.6477609653275563|\n",
      "|   LightGBM_Balanced|0.6627436979611613|0.6341211683352255|0.7231050469058541|0.8560357706973183|0.6627436979611613| 0.7380140465349575|0.7017172349648702|0.8373573581348537|0.6341211683352255| 0.6394412191278451|\n",
      "|      LightGBM_Tuned|0.8852308516199778|0.8888430178552998|0.8330344836565944|0.8636381237895965|0.8852308516199777| 0.6751312970353288|0.8381628224811306|0.8412936645810558|0.8888430178552998| 0.6488243631264718|\n",
      "+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LightGBM Tuned\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lgb_proxy = GBTClassifier(featuresCol=\"features\", labelCol=\"readmitted\", seed=1)\n",
    "\n",
    "#hyperparameter grid\n",
    "param_grid_lgb = ParamGridBuilder() \\\n",
    "    .addGrid(lgb_proxy.maxDepth, [3,5,7]) \\\n",
    "    .addGrid(lgb_proxy.maxIter, [50,100,150]) \\\n",
    "    .addGrid(lgb_proxy.stepSize, [0.05,0.1,0.2]) \\\n",
    "    .build()\n",
    "\n",
    "#evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"readmitted\", rawPredictionCol=\"rawPrediction\", \n",
    "                                          metricName=\"areaUnderROC\")\n",
    "#cross validation\n",
    "cv_lgb = CrossValidator(\n",
    "    estimator=lgb_proxy,\n",
    "    estimatorParamMaps=param_grid_lgb,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# Fit cross-validated model\n",
    "cv_lgb_model = cv_lgb.fit(train_data)\n",
    "best_lgb = cv_lgb_model.bestModel\n",
    "\n",
    "#model evaluation\n",
    "lgb_tuned_metrics = train_and_evaluate(cv_lgb_model.bestModel, train_data, test_data, \"LightGBM_Tuned\")\n",
    "# Append results\n",
    "scores = scores.union(spark.createDataFrame([Row(**lgb_tuned_metrics)]))\n",
    "scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8315582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputs will be saved in: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\n"
     ]
    }
   ],
   "source": [
    "# Save model outputs directory\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Use your base directory\n",
    "base_dir = r\"C:\\Projects\\hospital_readmission_prediction\\model\"\n",
    "\n",
    "# Ensure it exists\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Timestamped subdirectory for each training run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = os.path.join(base_dir, f\"run_{timestamp}\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model outputs will be saved in: {model_dir}\")\n",
    "\n",
    "\n",
    "# keep only the last 3 runs\n",
    "max_runs_to_keep = 3\n",
    "runs = sorted(glob.glob(os.path.join(base_dir, \"run_*\")), key=os.path.getmtime, reverse=True)\n",
    "for old_run in runs[max_runs_to_keep:]:\n",
    "    shutil.rmtree(old_run)\n",
    "    print(f\"Deleted old run folder: {old_run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0ceb491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of collected scores:\n",
      "               Model_Name  Train_Accuracy  Test_Accuracy  Train_f1  \\\n",
      "0      LogisticRegression        0.883777       0.887811  0.831041   \n",
      "1    LogisticReg_Balanced        0.641323       0.640778  0.705291   \n",
      "2  LogisticReg_Unbalanced        0.883828       0.888069  0.830679   \n",
      "3            DecisionTree        0.884925       0.888740  0.831818   \n",
      "4   DecisionTree_Balanced        0.644842       0.638817  0.708149   \n",
      "\n",
      "   Train_precision  Train_recall  Train_auc_roc   Test_f1  Test_precision  \\\n",
      "0         0.817044      0.883777       0.645914  0.837344        0.818970   \n",
      "1         0.832652      0.641323       0.648693  0.706805        0.835341   \n",
      "2         0.811450      0.883828       0.646203  0.836987        0.814757   \n",
      "3         0.864038      0.884925       0.393643  0.837227        0.829683   \n",
      "4         0.835023      0.644842       0.398117  0.705283        0.835237   \n",
      "\n",
      "   Test_recall  Test_auc_roc  \n",
      "0     0.887811      0.635276  \n",
      "1     0.640778      0.637555  \n",
      "2     0.888069      0.635428  \n",
      "3     0.888740      0.397486  \n",
      "4     0.638817      0.400603  \n",
      "Saved CSV: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\model_performance.csv\n",
      "Saved Excel: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\model_performance.xlsx\n"
     ]
    }
   ],
   "source": [
    "#%pip install openpyxl\n",
    "\n",
    "#Save model performance metrics to CSV and Excel\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the model directory exists\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# 1 Convert Spark DataFrame  list of dicts  Pandas DataFrame\n",
    "rows = scores.collect()  # list of Row objects\n",
    "scores_pd = pd.DataFrame([row.asDict() for row in rows])\n",
    "print(\"Sample of collected scores:\")\n",
    "print(scores_pd.head())\n",
    "\n",
    "# 2 Save as CSV\n",
    "csv_path = os.path.join(model_dir, \"model_performance.csv\")\n",
    "scores_pd.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "# 3 Save as Excel with multiple sheets\n",
    "excel_path = os.path.join(model_dir, \"model_performance.xlsx\")\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    \n",
    "    # Sheet 1: Raw metrics\n",
    "    scores_pd.to_excel(writer, sheet_name=\"Performance_Metrics\", index=False)\n",
    "    \n",
    "    # Sheet 2: Ranked summary by Test Accuracy\n",
    "    ranked_df = scores_pd.sort_values(\"Test_Accuracy\", ascending=False).reset_index(drop=True)\n",
    "    ranked_df[\"Rank\"] = range(1, len(ranked_df)+1)\n",
    "    ranked_df[[\"Rank\", \"Model_Name\", \"Test_Accuracy\", \"Test_f1\", \"Test_auc_roc\"]].to_excel(\n",
    "        writer, sheet_name=\"Rankings\", index=False\n",
    "    )\n",
    "    \n",
    "    # Sheet 3: Analysis / summary statistics\n",
    "    analysis_data = {\n",
    "        \"Metric\": [\n",
    "            \"Best Test Accuracy\",\n",
    "            \"Best F1 Score\",\n",
    "            \"Best AUC-ROC\",\n",
    "            \"Average Accuracy\",\n",
    "            \"Models Trained\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            f\"{scores_pd['Test_Accuracy'].max():.4f} ({scores_pd.loc[scores_pd['Test_Accuracy'].idxmax(), 'Model_Name']})\",\n",
    "            f\"{scores_pd['Test_f1'].max():.4f} ({scores_pd.loc[scores_pd['Test_f1'].idxmax(), 'Model_Name']})\",\n",
    "            f\"{scores_pd['Test_auc_roc'].max():.4f} ({scores_pd.loc[scores_pd['Test_auc_roc'].idxmax(), 'Model_Name']})\",\n",
    "            f\"{scores_pd['Test_Accuracy'].mean():.4f}\",\n",
    "            len(scores_pd)\n",
    "        ]\n",
    "    }\n",
    "    pd.DataFrame(analysis_data).to_excel(writer, sheet_name=\"Analysis\", index=False)\n",
    "\n",
    "print(f\"Saved Excel: {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca5187a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved performance report: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\performance_report.txt\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE PERFORMANCE REPORT (TEXT) ---\n",
    "performance_report = os.path.join(model_dir, \"performance_report.txt\")\n",
    "\n",
    "with open(performance_report, \"w\") as f:\n",
    "    f.write(\"Hospital Readmission Prediction - Model Performance Report\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    f.write(f\"Dataset Size: {df.count():,}\\n\")\n",
    "    f.write(f\"Training Set: {train_data.count():,}\\n\")\n",
    "    f.write(f\"Test Set: {test_data.count():,}\\n\\n\")\n",
    "    f.write(f\"Models Trained: {len(scores_pd)}\\n\\n\")\n",
    "    f.write(\"Class Distribution:\\n\")\n",
    "    f.write(f\"  Not Readmitted (0): {not_readmitted:,} ({(1-readmitted_rate)*100:.1f}%)\\n\")\n",
    "    f.write(f\"  Readmitted (1): {readmitted:,} ({readmitted_rate*100:.1f}%)\\n\")\n",
    "    f.write(f\"  Imbalance Ratio: {not_readmitted/readmitted:.2f}:1\\n\\n\")\n",
    "\n",
    "print(f\"Saved performance report: {performance_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved test dataset (CSV) with inferred feature names: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\test_data.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Infer feature names from the first row of the test dataset\n",
    "num_features = len(test_data.select(\"features\").first()[\"features\"])\n",
    "feature_names = [f\"feature_{i}\" for i in range(num_features)]\n",
    "\n",
    "# Conversion function\n",
    "def spark_to_pandas(df, feature_col=\"features\", target_col=\"readmitted\"):\n",
    "    pdf_features = pd.DataFrame(\n",
    "        [row[feature_col].toArray() for row in df.select(feature_col).collect()],\n",
    "        columns=feature_names\n",
    "    )\n",
    "    pdf_target = pd.DataFrame(\n",
    "        [row[target_col] for row in df.select(target_col).collect()],\n",
    "        columns=[target_col]\n",
    "    )\n",
    "    return pd.concat([pdf_features, pdf_target], axis=1)\n",
    "\n",
    "# Convert test_data\n",
    "test_data_pd = spark_to_pandas(test_data)\n",
    "\n",
    "# Save as CSV\n",
    "test_data_path = os.path.join(model_dir, \"test_data.csv\")\n",
    "test_data_pd.to_csv(test_data_path, index=False)\n",
    "print(f\"Saved test dataset (CSV) with inferred feature names: {test_data_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "518dcf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved XGBoost_Tuned metadata to: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\xgboost_tuned_model\n",
      " Saved LightGBM_Tuned metadata to: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\lightgbm_tuned_model\n",
      " Saved RandomForest_Tuned metadata to: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\random_forest_tuned_model\n",
      " Saved DecisionTree_Tuned metadata to: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\decision_tree_tuned_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --- Alternative model saving approach ---\n",
    "def save_model_metadata(model, model_path, model_name):\n",
    "    \"\"\"Save model metadata and parameters instead of the full model\"\"\"\n",
    "    try:\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "        # Extract model parameters\n",
    "        model_params = {\n",
    "            \"model_type\": str(type(model).__name__),\n",
    "            \"params\": {}\n",
    "        }\n",
    "        \n",
    "        # Get all parameters\n",
    "        for param in model.params:\n",
    "            try:\n",
    "                value = model.getOrDefault(param)\n",
    "                # Convert to JSON-serializable format\n",
    "                if hasattr(value, 'tolist'):\n",
    "                    value = value.tolist()\n",
    "                elif not isinstance(value, (str, int, float, bool, list, dict, type(None))):\n",
    "                    value = str(value)\n",
    "                model_params[\"params\"][param.name] = value\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Save model metadata\n",
    "        metadata_path = os.path.join(model_path, \"model_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(model_params, f, indent=2)\n",
    "        \n",
    "        # Save feature importance if available\n",
    "        if hasattr(model, 'featureImportances'):\n",
    "            importance_path = os.path.join(model_path, \"feature_importance.json\")\n",
    "            try:\n",
    "                importances = model.featureImportances.toArray().tolist()\n",
    "                with open(importance_path, 'w') as f:\n",
    "                    json.dump(importances, f)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\" Saved {model_name} metadata to: {model_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error saving {model_name} metadata: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Save model metadata ---\n",
    "xgb_model_path = os.path.join(model_dir, \"xgboost_tuned_model\")\n",
    "lgb_model_path = os.path.join(model_dir, \"lightgbm_tuned_model\")\n",
    "\n",
    "# Save XGBoost metadata\n",
    "save_model_metadata(best_xgb, xgb_model_path, \"XGBoost_Tuned\")\n",
    "\n",
    "# Save LightGBM metadata  \n",
    "save_model_metadata(best_lgb, lgb_model_path, \"LightGBM_Tuned\")\n",
    "\n",
    "# Save additional models if available\n",
    "if 'best_rf' in locals():\n",
    "    rf_model_path = os.path.join(model_dir, \"random_forest_tuned_model\")\n",
    "    save_model_metadata(best_rf, rf_model_path, \"RandomForest_Tuned\")\n",
    "\n",
    "if 'best_dt' in locals():\n",
    "    dt_model_path = os.path.join(model_dir, \"decision_tree_tuned_model\")\n",
    "    save_model_metadata(best_dt, dt_model_path, \"DecisionTree_Tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dd56cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved JSON model info: C:\\Projects\\hospital_readmission_prediction\\model\\run_20251113_173739\\model_info.json\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE MODEL INFO METADATA (JSON) ---\n",
    "import json\n",
    "\n",
    "# Rank models by accuracy\n",
    "ranked_scores = scores_pd.sort_values(\"Test_Accuracy\", ascending=False).to_dict(\"records\")\n",
    "\n",
    "model_info = {\n",
    "    \"project_info\": {\n",
    "        \"name\": \"Hospital Readmission Prediction\",\n",
    "        \"timestamp\": timestamp,\n",
    "        \"dataset_size\": df.count(),\n",
    "        \"train_size\": train_data.count(),\n",
    "        \"test_size\": test_data.count(),\n",
    "        \"models_trained\": len(scores_pd)\n",
    "    },\n",
    "    \"class_distribution\": {\n",
    "        \"not_readmitted\": int(not_readmitted),\n",
    "        \"readmitted\": int(readmitted),\n",
    "        \"imbalance_ratio\": float(not_readmitted / readmitted)\n",
    "    },\n",
    "    \"best_model\": ranked_scores[0],\n",
    "    \"all_models\": ranked_scores\n",
    "}\n",
    "\n",
    "info_path = os.path.join(model_dir, \"model_info.json\")\n",
    "with open(info_path, \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\" Saved JSON model info: {info_path}\")\n"
   ]
  }
 ],
=======
 "cells": [],
>>>>>>> Stashed changes
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
<<<<<<< Updated upstream
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
=======
   "name": "python",
>>>>>>> Stashed changes
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
